{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can we build a python via PubMed so that we can provide a direct option to bulk-extract literature texts into an Excel file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/weizhikexuejia/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/weizhikexuejia/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'requests' library is installed.\n",
      "The 'beautifulsoup4' library is installed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Check if requests is installed\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    print(\"The 'requests' library is not installed.\")\n",
    "else:\n",
    "    print(\"The 'requests' library is installed.\")\n",
    "\n",
    "# Check if beautifulsoup4 is installed\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    print(\"The 'beautifulsoup4' library is not installed.\")\n",
    "else:\n",
    "    print(\"The 'beautifulsoup4' library is installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "search_url = 'https://pubmed.ncbi.nlm.nih.gov/?term='\n",
    "search_term = '(ex-gaussian[Title/Abstract]) AND (reaction time[Title/Abstract])'\n",
    "results_per_page = 10  # Number of results per page\n",
    "num_pages = 6  # Number of pages to retrieve (example: 6 pages * 10 results per page = 60 results)\n",
    "\n",
    "results = []\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    # Send a request to PubMed for each page\n",
    "    response = requests.get(search_url + search_term + f'&page={page}')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the search results from the current page\n",
    "    page_results = soup.find_all('div', class_='docsum-content')\n",
    "    \n",
    "    # Add the page_results to the main results list\n",
    "    results.extend(page_results)\n",
    "\n",
    "print(len(results))  # Print the total number of search results retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, authors, years, abstracts, links = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(titles), len(authors), len(years), len(abstracts))\n",
    "\n",
    "titles = titles[:min_length]  \n",
    "authors = authors[:min_length]\n",
    "years = years[:min_length]\n",
    "abstracts = abstracts[:min_length] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for result in results:\n",
    "\n",
    "  title = result.find('a', class_='docsum-title').text\n",
    "  titles.append(title)\n",
    "\n",
    "  author = result.find('span', class_='docsum-authors').text\n",
    "  authors.append(author)\n",
    "\n",
    "  year = re.search(r'\\d{4}', result.find('span', class_='docsum-journal-citation').text).group()\n",
    "  years.append(year)\n",
    "\n",
    "  link = result.find('a', class_='docsum-title')['href']\n",
    "  links.append(link)\n",
    "\n",
    "  page = requests.get(f\"https://pubmed.ncbi.nlm.nih.gov{link}\")\n",
    "  soup = BeautifulSoup(page.content, 'html.parser')\n",
    "  abstract = soup.find('div', class_='abstract-content').text\n",
    "  abstracts.append(abstract)\n",
    "\n",
    "min_len = min(len(titles), len(authors), len(years), len(abstracts), len(links))  \n",
    "\n",
    "# Truncate lists\n",
    "\n",
    "titles = titles[:min_len]\n",
    "authors = authors[:min_len]\n",
    "years = years[:min_len]\n",
    "abstracts = abstracts[:min_len]\n",
    "links = links[:min_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Author': authors,\n",
    "    'Year': years,\n",
    "    'Abstract': abstracts,\n",
    "    'Link': links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('search_results.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
